"h  module.eks.kubernetes_config_map.aws_auth[0], │ on .terraform/modules/eks/aws_auth.tf line 63, in resource "kubernetes_config_map" "aws_auth": │ 63: resource "kubernetes_config_map" "aws_auth" {


c#Workflow in githib actions
name: Microservice pipeline App
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      destroy:
        description: "Destroy Resources"
        required: true
        default: "no"

jobs:
  containerization:
    runs-on: ubuntu-latest
    steps:
      - name: checkout config files
        uses: actions/checkout@v2
      - name: docker hub login
        uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}
      - name: install Dependencies
        run: |
          chmod +x install_dep.sh
          ./install_dep.sh
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          tags: ${{ vars.DOCKERHUB_USERNAME }}/micro-app:${{ github.sha }}
      - name: Docker image Test
        run: |
          docker images
          docker run --name micro-app -d -p 5000:5000 ${{ vars.DOCKERHUB_USERNAME }}/micro-app:${{ github.sha }}
          export IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' micro-app)
          echo $IP

  terraform:
    name: terraform deployment
    runs-on: ubuntu-latest
    needs: containerization
    environment: production
    steps:
      - name: checkout config files
        uses: actions/checkout@v2

      - name: Set up AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: setup terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.4.4
      - name: terraform init
        run: terraform init
        working-directory: ./Terraform
      
      - name: terraform plan
        run: terraform plan
        working-directory: ./Terraform

      - name: terraform apply or destroy
        run: |
          if [ "${{ github.event.inputs.destroy }}" = "yes" ]; then
            terraform destroy -auto-approve
          else
            terraform apply -auto-approve
          fi
        working-directory: ./Terraform
   
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest
    needs: terraform
    steps:
      - name: Checkout Kubernetes Manifests
        uses: actions/checkout@v2

      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Update kubeconfig
        run: |
              aws eks --region us-west-2 update-kubeconfig --name stage_eks

      - name: Trigger app deployment
        uses: statsig-io/kubectl-via-eksctl@main
        env:
            aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
            aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            region: us-west-2
            cluster: stage_eks
        with:
            args: rollout restart deployment/willanyoneeverreadthis

      - name: Deploy Kubernetes Manifests
        run: |
          kubectl apply -f deployment.yml  
          kubectl apply -f service.yml
        working-directory: ./kubernetes

      - name: Verify Deployment
        run: |
          kubectl get pods  
#-------------------------------------

#main.tf

terraform {
  backend "s3" {
    bucket         = "stagebucket12"
    key            = "stage-eks7/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-lock"
    encrypt        = true
  }
}


provider "aws" {
  region = var.region
}

module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  name   = "stage-eks-vpc1"
  cidr   = var.vpc_cidr

  enable_dns_support   = true
  enable_dns_hostnames = true
  single_nat_gateway   = false

  private_subnets = var.private_subnets
  public_subnets  = var.public_subnets
  azs             = ["us-west-2a", "us-west-2b"]
}


# Removed redundant Internet Gateway resource
# aws_internet_gateway is managed by the VPC module now

# Removed the explicit route table, route table association to avoid conflict with the VPC module
# The VPC module will automatically create these resources

resource "aws_security_group" "eks_sg" {
  name        = "stage-eks-sg"
  description = "Security group for EKS cluster"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 0
    to_port     = 5000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
 
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}


resource "aws_iam_role" "eks_worker_node_role" {
  name = "stage-eks-worker-node-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action    = "sts:AssumeRole"
        Effect    = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "worker_node_policy" {
  role       = aws_iam_role.eks_worker_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "worker_cni_policy" {
  role       = aws_iam_role.eks_worker_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "ec2_read_only_policy" {
  role       = aws_iam_role.eks_worker_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role" "eks_cluster_role" {
  name = "stage-eks-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action    = "sts:AssumeRole"
        Effect    = "Allow"
        Principal = {
          Service = "eks.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_eks_cluster" "stage_eks" {
  name     = var.cluster_name
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = module.vpc.private_subnets
  }
}

resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role_policy_attachment" "eks_service_policy" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
}

resource "aws_eks_node_group" "stage_eks_node_group" {
  cluster_name    = aws_eks_cluster.stage_eks.name
  node_group_name = "stage-eks-node-group"
  node_role_arn   = aws_iam_role.eks_worker_node_role.arn
  subnet_ids      = module.vpc.private_subnets

  scaling_config {
    desired_size = var.node_desired_capacity
    max_size     = var.node_max_capacity
    min_size     = var.node_min_capacity
  }

  depends_on = [
    aws_iam_role.eks_worker_node_role,
  ]
}

#------------------

#outputa.tf
output "eks_cluster_name" {
  value = aws_eks_cluster.stage_eks.name
}

output "eks_cluster_endpoint" {
  value = aws_eks_cluster.stage_eks.endpoint
}

output "vpc_id" {
  value = module.vpc.vpc_id
}

output "private_subnet_ids" {
  value = module.vpc.private_subnets
}

output "public_subnet_ids" {
  value = module.vpc.public_subnets
}
#----------------
#variables.tf

variable "region" {
  description = "AWS region to deploy resources in"
  type        = string
}

variable "vpc_cidr" {
  description = "CIDR block for the VPC"
  type        = string
}

variable "private_subnets" {
  description = "Private subnet CIDR blocks"
  type        = list(string)
}

variable "public_subnets" {
  description = "Public subnet CIDR blocks"
  type        = list(string)
}

variable "cluster_name" {
  description = "The name of the EKS cluster"
  type        = string
}

variable "node_instance_type" {
  description = "The instance type for the EKS nodes"
  type        = string
}

variable "node_desired_capacity" {
  description = "The desired number of worker nodes"
  type        = number
}

variable "node_max_capacity" {
  description = "The maximum number of worker nodes"
  type        = number
}

variable "node_min_capacity" {
  description = "The minimum number of worker nodes"
  type        = number
}

#-------------------
#terraform.tfvars

region              = "us-west-2"
vpc_cidr            = "10.0.0.0/16"
private_subnets     = ["10.0.1.0/24", "10.0.2.0/24"]
public_subnets      = ["10.0.3.0/24", "10.0.4.0/24"]
cluster_name        = "stage-eks-cluster"
node_instance_type  = "t3.medium"
node_desired_capacity = 2
node_max_capacity   = 4
node_min_capacity   = 1

#---------------
#deloyment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: microservice
spec:
  replicas: 2
  selector:
    matchLabels:
      app: microservice
  template:
    metadata:
      labels:
        app: microservice
    spec:
      containers:
      - name: microservice
        image: 20150748/micro-app:db451f1e251d730ff3f8eff4a06365bafe00ad8f
        ports:
        - containerPort: 5000
#----------------

#service.yml
apiVersion: v1
kind: Service
metadata:
  name: microservice-service
spec:
  selector:
    app: microservice
  ports:
  - protocol: TCP
    port: 5000  
    targetPort: 5000  
  type: LoadBalancer  
